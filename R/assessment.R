#' In-Sample Error
#'
#' @description
#' This function computes training error, in-sample error, and optimism in MC simulation setting.
#' @param data MC data set generated by \code{\link{mc_data}}.
#' @param ny the number of response at each x point
#' @param fit True model function with \code{x}-named argument.
#' @param rand Random sample generator function for error term. By default, \link[stats]{rnorm}
#' @param error Choice of loss function. See \code{\link{loss}}.
#' @param mod Model function.
#' @param formula an object of class \link[stats]{formula}.
#' @param ... Additional arguments for \code{mod}. If you wand argument for \code{rand}, define one.
#' @return
#' Training error, Insample error, Optimism
#' @details
#' In-sample error differs from Expected test error in that it is computed in the same predictor values.
#' Instead, it uses new response values at each predictor point.
#' \deqn{Err_{in} = \frac{1}{N} \sum_{i = 1}^N E_{y_0} [L(Y_i^{(0)}, \hat{f}(x_i)) \mid T]}
#' Optimism is the difference between the insample error and the training error.
#' @references Hastie, T., Tibshirani, R.,, Friedman, J. (2001). \emph{The Elements of Statistical Learning}. New York, NY, USA: Springer New York Inc..
#' @export
compute_insample <- function(data, ny, fit, rand, error = c("squared", "absolute"), mod, formula, ...) {
  error = match.arg(error)
  if (!("y" %in% names(data))) data <- gen_y(data, fit, rand, fit_col = FALSE)
  cols <- paste0("y", 1:ny)
  for (col in cols) {
    data[,
         (col) := fit(x) + rand(.N)]
  }
  data <- pred_dt(data = data, mod = mod, formula = formula, ...)
  data %>%
    melt(id.vars = c("x", "y", "mc", "pred")) %>%
    .[,
      .(
        training = loss(y, pred, error),
        insample = loss(value, pred, error)
      ),
      by = mc] %>%
    .[,
      optimism := insample - training] %>%
    .[,
      lapply(.SD, mean),
      .SDcols = -"mc"]
}

#' Generalization Error
#'
#' @description
#' This function computes expected test error in Monte Carlo simultion situation.
#' @param data MC data set generated by \code{\link{mc_data}}.
#' @param randx Random sample generator function for \code{x}. By default, \link[stats]{rnorm}
#' @param testn Test sample size
#' @param fit True model function with \code{x}-named argument.
#' @param randy Random sample generator function for error term. By default, \link[stats]{rnorm}
#' @param error Choice of loss function. See \code{\link{loss}}.
#' @param distribution return the error for each MC sample? \code{FALSE} by default. If \code{TRUE}, it gives the \code{data.table}.
#' @param mod Model function.
#' @param formula an object of class \link[stats]{formula}.
#' @param ... Additional arguments for \code{mod}. If you wand argument for \code{randx} or \code{randy}, define one.
#' @return
#' Expected test error
#' @details
#' Given MC samples, compute test error using independent test set and average.
#' @references Hastie, T., Tibshirani, R.,, Friedman, J. (2001). \emph{The Elements of Statistical Learning}. New York, NY, USA: Springer New York Inc..
#' @export
compute_epe <- function(data, randx, testn, fit, randy, error = c("squared", "absolute"), distribution = FALSE, mod, formula, ...) {
  error <- match.arg(error)
  test_set <-
    data.table(x = randx(n = testn)) %>%
    .[,
      y := fit(x) + randy(.N, ...)]
  data <-
    data %>%
    .[,
      y := fit(x) + randy(.N, ...),
      by = mc] %>%
    .[,
      .(pred = mod(y ~ x, data = .SD) %>%
          predict(test_set)),
      by = mc] %>%
    .[,
      y := test_set[, y],
      by = mc] %>%
    .[,
      .(error = loss(y, pred)),
      by = mc]
  if (distribution) {
    data
  } else {
    data %>%
      .[,
        error] %>%
      mean()
  }
}
